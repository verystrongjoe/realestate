{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy  # tensor copy용\n",
    "import numpy as np # numpy로만 rnn구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x) :\n",
    "    output = 1 / (1+np.exp(-x))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_output_to_derivative(output) :\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8190"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_output_to_derivative(-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2, binary_dim)\n",
    "\n",
    "largest_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ..., \n",
       "       [1, 1, 1, ..., 1, 0, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
    "binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(largest_number) :\n",
    "    int2binary[i] = binary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input variables\n",
    "alpha = 0.1 #학습률\n",
    "input_dim = 2  # a+ b-> input value\n",
    "hidden_dim = 16 # hidden layer 16\n",
    "output_dim = 1  # a+b = c -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 16)\n",
      "(16, 1)\n",
      "(16, 16)\n"
     ]
    }
   ],
   "source": [
    "# initialize neural network weights\n",
    "synapse_0 = 2 * np.random.random((input_dim, hidden_dim)) -1\n",
    "synapse_1 = 2 * np.random.random((hidden_dim, output_dim)) - 1\n",
    "synapse_h = 2 * np.random.random((hidden_dim, hidden_dim)) - 1\n",
    "print(synapse_0.shape)\n",
    "print(synapse_1.shape)\n",
    "print(synapse_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00515377520732012"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(0.9,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid_output_to_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#별도로 시냅스 업데이트 정보를 임시로 저장하기 위한 변수\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./data/layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[ 4.88375701]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "52 + 77 = 255\n",
      "------------\n",
      "Error:[ 3.98479372]\n",
      "Pred:[1 1 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 1 0 1 0]\n",
      "66 + 104 = 192\n",
      "------------\n",
      "Error:[ 3.60347871]\n",
      "Pred:[0 0 1 1 1 1 1 1]\n",
      "True:[0 0 0 1 1 1 1 1]\n",
      "23 + 8 = 63\n",
      "------------\n",
      "Error:[ 3.63916913]\n",
      "Pred:[1 1 0 1 1 0 1 1]\n",
      "True:[1 0 0 1 1 0 1 1]\n",
      "46 + 109 = 219\n",
      "------------\n",
      "Error:[ 2.89716074]\n",
      "Pred:[1 0 1 1 1 1 0 1]\n",
      "True:[1 0 1 1 1 1 0 1]\n",
      "110 + 79 = 189\n",
      "------------\n",
      "Error:[ 2.27077671]\n",
      "Pred:[1 0 1 0 0 1 0 0]\n",
      "True:[1 0 1 0 0 1 0 0]\n",
      "122 + 42 = 164\n",
      "------------\n",
      "Error:[ 0.85480656]\n",
      "Pred:[0 0 0 1 1 1 1 1]\n",
      "True:[0 0 0 1 1 1 1 1]\n",
      "25 + 6 = 31\n",
      "------------\n",
      "Error:[ 1.21367149]\n",
      "Pred:[1 0 0 0 1 0 0 0]\n",
      "True:[1 0 0 0 1 0 0 0]\n",
      "28 + 108 = 136\n",
      "------------\n",
      "Error:[ 0.56660859]\n",
      "Pred:[1 0 0 1 1 0 1 1]\n",
      "True:[1 0 0 1 1 0 1 1]\n",
      "67 + 88 = 155\n",
      "------------\n",
      "Error:[ 0.55687411]\n",
      "Pred:[1 1 1 1 0 1 0 1]\n",
      "True:[1 1 1 1 0 1 0 1]\n",
      "121 + 124 = 245\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# 1만번 학습\n",
    "for j in range(10000):\n",
    "\n",
    "    # a + b = c   \n",
    "    # a,b는 random 생성\n",
    "    a_int = np.random.randint(largest_number/2)\n",
    "    a = int2binary[a_int]\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2)\n",
    "    b = int2binary[b_int]\n",
    "\n",
    "    # 실제 결과값\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "\n",
    "    # prediction value 저장 변수\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "\n",
    "    # layer1 ( input -> hidden layer )\n",
    "    layer_1_values = list()  # single value\n",
    "    # 초기엔 이전 상태의 hidden layer가 없기 때문에 0으로 초기화된 array를 별도로 저장\n",
    "    layer_1_values.append(np.zeros(hidden_dim)) # list of above\n",
    "    # layer2의 back props value를 보관\n",
    "    layer_2_deltas = list()\n",
    "\n",
    "    # forward propagation\n",
    "    for position in range(binary_dim) :\n",
    "\n",
    "        # genearte input and output Transpose 행렬 취해서 결과 포멧shape(1, 8)을 맞춰줌 \n",
    "        X = np.array( [[ a[binary_dim - position - 1], b[binary_dim - position - 1]]])\n",
    "        y = np.array( [[ c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)  그림1\n",
    "        layer_1 = sigmoid( np.dot(X, synapse_0) + np.dot(layer_1_values[-1], synapse_h) )\n",
    "\n",
    "        # output layer 여기는 post_hidden layer ->  output layer\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # 결과값과의 차이\n",
    "        layer_2_error = y - layer_2 \n",
    "        \n",
    "        # 결과값의 차이를 sigmoid\n",
    "        # layer 2의 델타 값은 sigmoid미분값(잔차)에 layer2_error를 곱함 (back props시 변경되어야 하는 변화량)\n",
    "        layer_2_deltas.append( (layer_2_error) * sigmoid_output_to_derivative(layer_2)) \n",
    "        #  overallError변수에 계속 델타값을 누적\n",
    "        # 여기에서 왜 리스트를 취하는걸까? single value인데 말이다 그리고 절대값을 취하는건 건 왜 인지 모르겠음..\n",
    "        overallError += np.abs(layer_2_error[0]) \n",
    "\n",
    "        # 예측값 저장\n",
    "        d[binary_dim - position -1 ] = np.round(layer_2[0][0]) # 0.5는 1인줄 알았더니 0으로 처리함\n",
    "\n",
    "        # RNN이므로 현재 상태의 hidden layer의 값을 다음 상태의 hidden layer에 input으로 사용하기 위해 저장\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "\n",
    "    # 이 변수는 back prop시에도 마찬가지로 마지막 다음의 hidden layer를 별도로 상태\n",
    "    future_layer_1_delta  = np.zeros(hidden_dim)\n",
    "\n",
    "    # backpropagation 진행\n",
    "    for position in range(binary_dim) :\n",
    "        \n",
    "        X = np.array([[a[position], b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1] # 이전 layer1의 값\n",
    "        prev_layer_1= layer_1_values[-position-2] # 그 이전의 layer1의 값\n",
    "\n",
    "        # error of output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1] # layer2의 delta도 가져옴\n",
    "\n",
    "        # error of hidden layer\n",
    "        layer_1_delta = (  future_layer_1_delta.dot(synapse_h.T)\n",
    "                            + layer_2_delta.dot(synapse_1.T)\n",
    "                        ) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # delta값을 이용하여 시냅스의 weight를 조정하는 값을 계산\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "\n",
    "        # 현재 상태의 이용한 delta값을 저장 \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "\n",
    "    # alpha는 학습률이며 현재 시냅스를 실제 조정하는 부분\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha\n",
    "\n",
    "    # 초기화\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "\n",
    "    # 1000번 돌때마다 overallError를 계산\n",
    "    if (j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
